{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Part 3\n\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Matthew-Bustamante/CS39AA-Project-Cyberbullying/blob/main/project_part3.ipynb)\n\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Matthew-Bustamante/CS39AA-Project-Cyberbullying/blob/main/project_part3.ipynb)\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch.nn.functional as F\nimport torch.cuda\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T18:45:09.892545Z","iopub.execute_input":"2023-11-30T18:45:09.892904Z","iopub.status.idle":"2023-11-30T18:45:09.899321Z","shell.execute_reply.started":"2023-11-30T18:45:09.892879Z","shell.execute_reply":"2023-11-30T18:45:09.898283Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"input_data_path = 'https://raw.githubusercontent.com/Matthew-Bustamante/CS39AA-Project-Cyberbullying/main/CyberBullying_Comments_Dataset.csv'\ndf = pd.read_csv(input_data_path)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:09.906951Z","iopub.execute_input":"2023-11-30T18:45:09.907448Z","iopub.status.idle":"2023-11-30T18:45:09.994582Z","shell.execute_reply.started":"2023-11-30T18:45:09.907412Z","shell.execute_reply":"2023-11-30T18:45:09.993592Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"                                                Text  CB_Label\n0  damn there is someones nana up here at beach w...         0\n1  no kidding! dick clark was a corpse mechanical...         0\n2  i read an article on jobros and thought damn w...         0\n3  I got one fucking day of sprinkles and now it'...         0\n4  I was already listening to Elliott smith  and ...         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>CB_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>damn there is someones nana up here at beach w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>no kidding! dick clark was a corpse mechanical...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i read an article on jobros and thought damn w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I got one fucking day of sprinkles and now it'...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I was already listening to Elliott smith  and ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline, AutoModelForCausalLM\nfrom datasets import Dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:09.995998Z","iopub.execute_input":"2023-11-30T18:45:09.996276Z","iopub.status.idle":"2023-11-30T18:45:10.000708Z","shell.execute_reply.started":"2023-11-30T18:45:09.996252Z","shell.execute_reply":"2023-11-30T18:45:09.999716Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"#MODEL_NAME = \"bert-base-cased\"\nMODEL_NAME = \"gpt2\"\nMAX_LENGTH=50 \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, max_length=MAX_LENGTH, output_attentions=False, output_hidden_states=False)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, num_labels=3, max_length=MAX_LENGTH, output_attentions=False, output_hidden_states=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:10.001838Z","iopub.execute_input":"2023-11-30T18:45:10.002091Z","iopub.status.idle":"2023-11-30T18:45:12.487743Z","shell.execute_reply.started":"2023-11-30T18:45:10.002068Z","shell.execute_reply":"2023-11-30T18:45:12.486943Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Creating Vocabulary for y(dependent variable).  Note that 0 = non-cyberbullying and 1 = cyberbullying","metadata":{}},{"cell_type":"code","source":"classes = df.CB_Label.unique().tolist()\nclass_tok2idx = dict((v, k) for k, v in enumerate(classes))\nclass_idx2tok = dict((k, v) for k, v in enumerate(classes))\nprint(class_tok2idx)\nprint(class_idx2tok)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.489653Z","iopub.execute_input":"2023-11-30T18:45:12.489948Z","iopub.status.idle":"2023-11-30T18:45:12.496276Z","shell.execute_reply.started":"2023-11-30T18:45:12.489923Z","shell.execute_reply":"2023-11-30T18:45:12.495241Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"{0: 0, 1: 1}\n{0: 0, 1: 1}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Creating a new column with these labels which will be the y that is used","metadata":{}},{"cell_type":"code","source":"df['label'] = df['CB_Label'].apply(lambda x: class_tok2idx[x])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.497312Z","iopub.execute_input":"2023-11-30T18:45:12.497632Z","iopub.status.idle":"2023-11-30T18:45:12.522052Z","shell.execute_reply.started":"2023-11-30T18:45:12.497606Z","shell.execute_reply":"2023-11-30T18:45:12.521165Z"},"trusted":true},"execution_count":129,"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"                                                Text  CB_Label  label\n0  damn there is someones nana up here at beach w...         0      0\n1  no kidding! dick clark was a corpse mechanical...         0      0\n2  i read an article on jobros and thought damn w...         0      0\n3  I got one fucking day of sprinkles and now it'...         0      0\n4  I was already listening to Elliott smith  and ...         0      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>CB_Label</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>damn there is someones nana up here at beach w...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>no kidding! dick clark was a corpse mechanical...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i read an article on jobros and thought damn w...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I got one fucking day of sprinkles and now it'...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I was already listening to Elliott smith  and ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\nsequence_0 = \"Damn  hope you getz better soon !!!\"\nseq0_tokens = tokenizer(sequence_0, return_tensors=\"pt\")\nprint(f\"number of tokens in seq0 is {len(seq0_tokens['input_ids'].flatten())}\")\nprint(seq0_tokens)\nF.softmax(model(**seq0_tokens).logits, dim=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.523112Z","iopub.execute_input":"2023-11-30T18:45:12.523461Z","iopub.status.idle":"2023-11-30T18:45:12.635383Z","shell.execute_reply.started":"2023-11-30T18:45:12.523426Z","shell.execute_reply":"2023-11-30T18:45:12.634523Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"number of tokens in seq0 is 10\n{'input_ids': tensor([[43343,   220,  2911,   345,   651,    89,  1365,  2582,   220, 10185]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"},{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n          1.0000e+00, 1.0000e+00],\n         [3.1927e-10, 1.3907e-10, 7.4000e-11,  ..., 4.0189e-12,\n          3.1584e-11, 3.7002e-12],\n         [4.8246e-26, 1.6801e-26, 2.5647e-27,  ..., 4.5410e-27,\n          3.6083e-27, 9.6959e-27],\n         ...,\n         [1.7994e-22, 2.6522e-23, 1.9690e-23,  ..., 2.6334e-24,\n          3.4669e-25, 1.2228e-22],\n         [9.0064e-22, 1.2230e-21, 8.9241e-22,  ..., 2.5080e-22,\n          2.1852e-22, 2.0343e-20],\n         [9.1879e-22, 9.9687e-22, 1.4542e-20,  ..., 4.2151e-23,\n          2.6981e-23, 3.4057e-19]]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"sequence_1 = \"Shakespeare nerd!\"\nseq0_tokens = tokenizer(sequence_1, return_tensors=\"pt\")\nprint(f\"number of tokens in seq1 is {len(seq0_tokens['input_ids'].flatten())}\")\nprint(seq0_tokens)\nF.softmax(model(**seq0_tokens).logits, dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.636571Z","iopub.execute_input":"2023-11-30T18:45:12.636841Z","iopub.status.idle":"2023-11-30T18:45:12.713708Z","shell.execute_reply.started":"2023-11-30T18:45:12.636816Z","shell.execute_reply":"2023-11-30T18:45:12.712681Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"number of tokens in seq1 is 4\n{'input_ids': tensor([[ 2484, 20946, 34712,     0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n","output_type":"stream"},{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n          1.0000e+00, 1.0000e+00],\n         [5.8101e-22, 6.5962e-22, 1.5302e-23,  ..., 1.0753e-21,\n          2.1912e-21, 3.5497e-22],\n         [3.2552e-15, 5.2959e-15, 6.4626e-17,  ..., 7.7948e-16,\n          2.1389e-16, 2.3901e-16],\n         [9.0530e-27, 2.5156e-27, 4.6379e-27,  ..., 3.5597e-28,\n          4.5759e-28, 2.4334e-25]]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"sequence_2 = \"Is there ever a day that mattresses are not on sale?\"\nseq0_tokens = tokenizer(sequence_2, return_tensors=\"pt\")\nprint(f\"number of tokens in seq2 is {len(seq0_tokens['input_ids'].flatten())}\")\nprint(seq0_tokens)\nF.softmax(model(**seq0_tokens).logits, dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.714984Z","iopub.execute_input":"2023-11-30T18:45:12.715610Z","iopub.status.idle":"2023-11-30T18:45:12.841191Z","shell.execute_reply.started":"2023-11-30T18:45:12.715574Z","shell.execute_reply":"2023-11-30T18:45:12.840050Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"number of tokens in seq2 is 13\n{'input_ids': tensor([[ 3792,   612,  1683,   257,  1110,   326, 23963, 16746,   389,   407,\n           319,  5466,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"},{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n          1.0000e+00, 1.0000e+00],\n         [3.3123e-25, 1.1844e-25, 1.3796e-26,  ..., 2.0306e-24,\n          2.1939e-26, 2.2639e-25],\n         [2.8305e-14, 2.3349e-14, 5.3661e-16,  ..., 1.9047e-14,\n          2.7685e-14, 3.0814e-14],\n         ...,\n         [3.8406e-25, 4.2285e-26, 1.0269e-28,  ..., 3.8795e-27,\n          5.6176e-27, 7.8150e-27],\n         [2.6529e-39, 4.0599e-40, 1.3692e-41,  ..., 2.8487e-41,\n          9.2864e-42, 1.3836e-40],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n          0.0000e+00, 4.2039e-45]]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"sequence_3 = \"Dood! You just justified your MBA. I pity the sorry ass of that downtrodden employee whose manager you'll soon become.\"\nseq0_tokens = tokenizer(sequence_3, return_tensors=\"pt\")\nprint(f\"number of tokens in seq3 is {len(seq0_tokens['input_ids'].flatten())}\")\nprint(seq0_tokens)\nF.softmax(model(**seq0_tokens).logits, dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.842535Z","iopub.execute_input":"2023-11-30T18:45:12.842847Z","iopub.status.idle":"2023-11-30T18:45:12.965920Z","shell.execute_reply.started":"2023-11-30T18:45:12.842819Z","shell.execute_reply":"2023-11-30T18:45:12.964823Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"number of tokens in seq3 is 27\n{'input_ids': tensor([[   35,   702,     0,   921,   655, 14460,   534, 45517,    13,   314,\n         26246,   262,  7926,   840,   286,   326,  8039,   305,  4742,  6538,\n          3025,  4706,   345,  1183,  2582,  1716,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1]])}\n","output_type":"stream"},{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n          1.0000e+00, 1.0000e+00],\n         [1.7504e-23, 1.3703e-23, 1.4603e-24,  ..., 2.2641e-23,\n          1.5879e-23, 3.6523e-23],\n         [5.6825e-25, 9.9847e-26, 6.2765e-25,  ..., 4.8282e-26,\n          8.1235e-26, 6.7659e-24],\n         ...,\n         [9.8091e-45, 1.1210e-44, 1.4013e-45,  ..., 0.0000e+00,\n          8.4078e-45, 2.8026e-45],\n         [2.8115e-26, 1.5521e-27, 6.8605e-29,  ..., 4.4561e-28,\n          6.6813e-28, 4.5381e-28],\n         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n          0.0000e+00, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"ds_raw = Dataset.from_pandas(df[['label', 'Text']])","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.969636Z","iopub.execute_input":"2023-11-30T18:45:12.969964Z","iopub.status.idle":"2023-11-30T18:45:12.984562Z","shell.execute_reply.started":"2023-11-30T18:45:12.969935Z","shell.execute_reply":"2023-11-30T18:45:12.983352Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\nds = ds_raw.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:12.985816Z","iopub.execute_input":"2023-11-30T18:45:12.986233Z","iopub.status.idle":"2023-11-30T18:45:13.742520Z","shell.execute_reply.started":"2023-11-30T18:45:12.986191Z","shell.execute_reply":"2023-11-30T18:45:13.741548Z"},"trusted":true},"execution_count":135,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f767f12d15a4b4daabf2501ba0b01b8"}},"metadata":{}}]},{"cell_type":"code","source":"ds[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.743813Z","iopub.execute_input":"2023-11-30T18:45:13.744129Z","iopub.status.idle":"2023-11-30T18:45:13.752636Z","shell.execute_reply.started":"2023-11-30T18:45:13.744100Z","shell.execute_reply":"2023-11-30T18:45:13.751536Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"{'label': 0,\n 'Text': 'damn there is someones nana up here at beach with one. dont think ic an steal and get to you quickly tho ;(',\n 'input_ids': [11043,\n  77,\n  612,\n  318,\n  617,\n  1952,\n  299,\n  2271,\n  510,\n  994,\n  379,\n  10481,\n  351,\n  530,\n  13,\n  17666,\n  892,\n  14158,\n  281,\n  8711,\n  290,\n  651,\n  284,\n  345,\n  2952,\n  42796,\n  2162,\n  7,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"code","source":"ds = ds.shuffle(seed=42)\nds[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.753793Z","iopub.execute_input":"2023-11-30T18:45:13.754081Z","iopub.status.idle":"2023-11-30T18:45:13.772415Z","shell.execute_reply.started":"2023-11-30T18:45:13.754056Z","shell.execute_reply":"2023-11-30T18:45:13.771518Z"},"trusted":true},"execution_count":137,"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"{'label': 0,\n 'Text': \"haha it was great... i hate how the audio lags so badly though... it'd be more fun in real time lol\",\n 'input_ids': [71,\n  12236,\n  340,\n  373,\n  1049,\n  986,\n  1312,\n  5465,\n  703,\n  262,\n  6597,\n  300,\n  3775,\n  523,\n  11234,\n  996,\n  986,\n  340,\n  1549,\n  307,\n  517,\n  1257,\n  287,\n  1103,\n  640,\n  19462,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256,\n  50256],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"code","source":"train_prop = 0.85\nds_train = ds.select(range(int(len(ds)*train_prop)))\nds_eval = ds.select(range(int(len(ds)*train_prop), len(ds)))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.773640Z","iopub.execute_input":"2023-11-30T18:45:13.773938Z","iopub.status.idle":"2023-11-30T18:45:13.784994Z","shell.execute_reply.started":"2023-11-30T18:45:13.773913Z","shell.execute_reply":"2023-11-30T18:45:13.784098Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"print(f\"len(ds_train) = {len(ds_train)}\")\nprint(f\"len(ds_eval) = {len(ds_eval)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.786093Z","iopub.execute_input":"2023-11-30T18:45:13.786346Z","iopub.status.idle":"2023-11-30T18:45:13.792990Z","shell.execute_reply.started":"2023-11-30T18:45:13.786324Z","shell.execute_reply":"2023-11-30T18:45:13.791952Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"len(ds_train) = 9435\nlen(ds_eval) = 1665\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.794017Z","iopub.execute_input":"2023-11-30T18:45:13.794295Z","iopub.status.idle":"2023-11-30T18:45:13.801854Z","shell.execute_reply.started":"2023-11-30T18:45:13.794270Z","shell.execute_reply":"2023-11-30T18:45:13.801120Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"metric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ntraining_args = TrainingArguments(num_train_epochs=10,\n                                  do_train=True,\n                                  report_to=None,\n                                  output_dir=\"/kaggle/working\",\n                                  evaluation_strategy=\"steps\",\n                                  eval_steps=200,\n                                  learning_rate=1e-5,\n                                  per_device_train_batch_size=32,\n                                  per_device_eval_batch_size=32)\n\ntrainer = Trainer(model = model, \n                  args = training_args,\n                  train_dataset = ds_train, \n                  eval_dataset = ds_eval,\n                  compute_metrics = compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:13.803322Z","iopub.execute_input":"2023-11-30T18:45:13.803765Z","iopub.status.idle":"2023-11-30T18:45:14.397118Z","shell.execute_reply.started":"2023-11-30T18:45:13.803734Z","shell.execute_reply":"2023-11-30T18:45:14.396312Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda:0\"\n    print(\"Using GPU\")\nelse:\n    device = \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:14.398155Z","iopub.execute_input":"2023-11-30T18:45:14.398416Z","iopub.status.idle":"2023-11-30T18:45:14.403218Z","shell.execute_reply.started":"2023-11-30T18:45:14.398393Z","shell.execute_reply":"2023-11-30T18:45:14.402362Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Using GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:14.404315Z","iopub.execute_input":"2023-11-30T18:45:14.404615Z","iopub.status.idle":"2023-11-30T18:45:14.418268Z","shell.execute_reply.started":"2023-11-30T18:45:14.404591Z","shell.execute_reply":"2023-11-30T18:45:14.417426Z"},"trusted":true},"execution_count":143,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"torch.set_grad_enabled(True)\ntrainer.train()\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:14.419403Z","iopub.execute_input":"2023-11-30T18:45:14.419735Z","iopub.status.idle":"2023-11-30T18:45:15.180688Z","shell.execute_reply.started":"2023-11-30T18:45:14.419704Z","shell.execute_reply":"2023-11-30T18:45:15.179387Z"},"trusted":true},"execution_count":144,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[144], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1107\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1107\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1110\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1568) to match target batch_size (31)."],"ename":"ValueError","evalue":"Expected input batch_size (1568) to match target batch_size (31).","output_type":"error"}]},{"cell_type":"code","source":"df_test = pd.read_csv(\"https://raw.githubusercontent.com/Matthew-Bustamante/CS39AA-Project-Cyberbullying/main/CyberBullying_Comments_Dataset.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.181404Z","iopub.status.idle":"2023-11-30T18:45:15.181762Z","shell.execute_reply.started":"2023-11-30T18:45:15.181577Z","shell.execute_reply":"2023-11-30T18:45:15.181594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test_raw = Dataset.from_pandas(df_test)\nds_test_raw[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.183307Z","iopub.status.idle":"2023-11-30T18:45:15.183790Z","shell.execute_reply.started":"2023-11-30T18:45:15.183541Z","shell.execute_reply":"2023-11-30T18:45:15.183563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = ds_test_raw.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.184946Z","iopub.status.idle":"2023-11-30T18:45:15.185411Z","shell.execute_reply.started":"2023-11-30T18:45:15.185158Z","shell.execute_reply":"2023-11-30T18:45:15.185180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = trainer.predict(test_dataset=ds_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.186394Z","iopub.status.idle":"2023-11-30T18:45:15.186829Z","shell.execute_reply.started":"2023-11-30T18:45:15.186599Z","shell.execute_reply":"2023-11-30T18:45:15.186621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.apply_along_axis(np.argmax, 1, preds.predictions)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.189090Z","iopub.status.idle":"2023-11-30T18:45:15.189472Z","shell.execute_reply.started":"2023-11-30T18:45:15.189273Z","shell.execute_reply":"2023-11-30T18:45:15.189290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['preds'] = test_preds.tolist()\ndf_test['CB_Label'] = df_test['preds'].apply(lambda x: class_idx2tok[x])\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T18:45:15.190833Z","iopub.status.idle":"2023-11-30T18:45:15.191179Z","shell.execute_reply.started":"2023-11-30T18:45:15.191006Z","shell.execute_reply":"2023-11-30T18:45:15.191023Z"},"trusted":true},"execution_count":null,"outputs":[]}]}